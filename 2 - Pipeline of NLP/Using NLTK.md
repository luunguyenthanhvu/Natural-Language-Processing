# Tiá»n sá»­ lÃ½ vÄƒn báº£n vá»›i NLTK

  - <b> Äá»c vÄƒn báº£n:</b>`text = f.read()`  
  - <b> TÃ¡ch cÃ¢u thÃ nh cÃ¡c token: </b> `nltk.word_tokenize`
  - <b> Loáº¡i bá» kÃ½ tá»± xuá»‘ng dÃ²ng:</b> `text.replace("\n", " ")`
  - <b> Biáº¿n Ä‘á»•i vá» chá»¯ thÆ°á»ng: </b>`text.lower()`
  - <b> GÃ¡n Part-Of-Speech (POS) tags (nhÃ£n tá»« loáº¡i): </b> `nltk.pos_tag`
      + sá»­ dá»¥ng Ä‘á»ƒ gÃ¡n Part-Of-Speech (POS) tags (nhÃ£n tá»« loáº¡i) cho cÃ¡c tá»« trong má»™t cÃ¢u hoáº·c má»™t Ä‘oáº¡n vÄƒn báº£n. POS tags giÃºp xÃ¡c Ä‘á»‹nh chá»©c nÄƒng ngá»¯ phÃ¡p cá»§a tá»«ng tá»«, cháº³ng háº¡n nhÆ° danh tá»«, Ä‘á»™ng tá»«, tÃ­nh tá»«, tráº¡ng tá»«, v.v.
      + VÃ­ dá»¥ vá» cÃ¡c POS tags:
	  - NN: Danh tá»« chung (Noun)
	  - VB: Äá»™ng tá»« gá»‘c (Verb)
	  - JJ: TÃ­nh tá»« (Adjective)
	  - RB: Tráº¡ng tá»« (Adverb)
	  - PRP: Äáº¡i tá»« (Pronoun)
	  - VÃ  nhiá»u nhÃ£n khÃ¡c... 
  - <b> TÃ¡ch cÃ¡c cÃ¢u:</b>
      + Tá»« má»™t Ä‘oáº¡n vÄƒn báº£n gá»“m nhiá»u cÃ¢u thÃ¬ thÃ´ng qua bÆ°á»›c nÃ y ta thu Ä‘Æ°á»£c cÃ¡c cÃ¢u thÃ nh pháº§n. Äá»ƒ nháº­n biáº¿t má»™t cÃ¢u Ä‘Æ¡n giáº£n nháº¥t lÃ  khi gáº·p dáº¥u "." káº¿t thÃºc cÃ¢u, chÃºng ta hoÃ n toÃ n cÃ³ thá»ƒ sá»­ dá»¥ng hÃ m split() trong python vÃ  tÃ¡ch cÃ¢u má»—i khi gáº·p dáº¥u ".". Tuy nhiÃªn, khÃ´ng pháº£i lÃºc nÃ o dáº¥u "." cÅ©ng lÃ  káº¿t thÃºc cÃ¢u, vÃ­ dá»¥ trong tiáº¿ng anh, tá»« "Mr. Smith" thÃ¬ náº¿u ta dÃ¹ng cÃ¡ch trÃªn Ä‘á»ƒ bÃ³c tÃ¡ch cÃ¡c cÃ¢u thÃ¬ sáº½ sai. Äá»ƒ cÃ³ thá»ƒ tÃ¡ch cÃ¡c cÃ¢u chÃ­nh xÃ¡c thÃ¬ viá»‡c sá»­ dá»¥ng cÃ¡c thÆ° viá»‡n há»— trá»£ lÃ  biá»‡n phÃ¡p Ä‘Æ¡n giáº£n nháº¥t cá»¥ thá»ƒ lÃ  sá»­ dá»¥ng hÃ m `nltk.sent_tokenize`.
      +   EX:
            ```bash
           sentence_list = nltk.sent_tokenize(raw_text)
            # Print some sentences
            for i in range(100, 110):
              print(sentence_list[i])
              print("-----")
            print("---------------------------------------")
            print(f"Number of sentences: {len(sentence_list)}")
            ```
      + BÃªn cáº¡nh hÃ m Ä‘á»ƒ tÃ¡ch cÃ¡c cÃ¢u thÃ¬ thÆ° viá»‡n NLTK cÅ©ng cung cáº¥p hÃ m Ä‘á»ƒ tÃ¡ch cÃ¡c tá»« `nltk.word_tokenize`.
  - <b> Loáº¡i bá» cÃ¡c kÃ­ tá»± Ä‘áº·c biá»‡t (dáº¥u cÃ¢u): </b>
      + Trong cÃ¡c cÃ¢u trong vÄƒn báº£n sáº½ tá»“n táº¡i nhiá»u dáº¥u cÃ¢u nhÆ° ?, !, ", ;, ... trÆ°á»›c khi xÃ¢y dá»±ng bá»™ tá»« vá»±ng thÃ¬ cÃ¡c kÃ­ tá»± nÃ y cÅ©ng cáº§n Ä‘Æ°á»£c loáº¡i bá». Python Ä‘Ã£ Ä‘á»‹nh nghÄ©a cÃ¡c dáº¥u cÃ¢u trong `string.punctuation` tuy vÃ o bÃ i toÃ¡n ta cÃ³ thá»ƒ thÃªm hoáº·c bá»›t cÃ¡c dáº¥u nÃ y cho phÃ¹ há»£p.
      + EX:
          ```bash
          import string
          print("Punctuations: " + string.punctuation + "\n")
          sentence_list = [sentence.translate(str.maketrans('', '', string.punctuation)) for sentence in sentence_list]
          
          for i in range(100, 110):
            print(sentence_list[i])
            print("-----")
          ```
      + KhÃ´ng chá»‰ dáº¥u "," vÃ  "." trong cÃ¢u Ä‘Ã£ Ä‘Æ°á»£c loáº¡i bá» thÃ¬ cÃ¡c dáº¥u Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong `string.punctuation` cÅ©ng Ä‘Æ°á»£c bá» Ä‘i. NgoÃ i ra ta cÃ³ thá»ƒ thÃªm bá»›t cÃ¡c dáº¥u tÃ¹y theo má»¥c Ä‘Ã­ch.
  - <b> Loáº¡i bá» stop-word: </b>
    + Stop words thÆ°á»ng lÃ  cÃ¡c tá»« xuáº¥t hiá»‡n nhiá»u láº§n vÃ  khÃ´ng Ä‘Ã³ng gÃ³p nhiá»u vÃ o Ã½ nghÄ©a cá»§a cÃ¢u, chÃºng sáº½ Ä‘Ã³ng vai trÃ² nhÆ° nhiá»…u, trong tiáº¿ng Anh cÃ¡c tá»« nÃ y cÃ³ thá»ƒ ká»ƒ Ä‘áº¿n nhÆ° the, is, at, on, which, in, some, many hay trong tiáº¿ng Viá»‡t lÃ  cÃ¡c tá»« cÃ¡i, cÃ¡c, cáº£,.... CÃ¡c tá»« nÃ y thÆ°á»ng sáº½ Ä‘Æ°á»£c loáº¡i bá» Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c cá»§a bá»™ tá»« vá»±ng. Trong thÆ° viá»‡n NLTK cÃ³ Ä‘á»‹nh nghÄ©a cÃ¡c stop words phá»• biáº¿n trong tiáº¿ng Anh, tuy nhiÃªn tÃ¹y thuá»™c vÃ o má»¥c Ä‘Ã­ch, bÃ i toÃ n mÃ  ta sáº½ thÃªm bá»›t cÃ¡c stop word cho phÃ¹ há»£p. Äá»ƒ sá»­ dá»¥ng stop words cá»§a NLTK, trÆ°á»›c tiÃªn ta cáº§n download bá»™ stop words `nltk.download('stopwords')`.
    + EX:
        ```bash
        nltk.download('stopwords')
        from nltk.corpus import stopwords
        
        stop_word_list = set(stopwords.words('english'))
        print(stop_word_list, "\n")
        for i in range(len(sentence_list)):
          sentence_list[i] = " ".join([word for word in sentence_list[i].split() if word not in stop_word_list])
        
        for i in range(100, 110):
          print(sentence_list[i])
          print("-----")

        ```
  - <b> Loáº¡i bá» cÃ¡c tá»« hiáº¿m gáº·p: </b>
      + BÃªn cáº¡nh stop words thÃ¬ trong nhiá»u bÃ i toÃ¡n ta cÅ©ng cáº§n pháº£i loáº¡i bá» cÃ¡c tá»« hiáº¿m gáº·p, Ä‘áº·c biá»‡t khi bá»™ dá»¯ liá»‡u lá»›n. ÄÃ¢y cÅ©ng lÃ  kÄ© thuáº­t mÃ  ta cáº§n cÃ¢n nháº¯c trÆ°á»›c khi sá»­ dá»¥ng bá»Ÿi loáº¡i bá» cÃ¡c tá»« nÃ y cÃ³ thá»ƒ gÃ¢y ra sá»± máº¥t mÃ¡t vá» ngá»¯ nghÄ©a cá»§a cÃ¢u, Ä‘áº·c biá»‡t lÃ  vá»›i cÃ¡c bá»™ dá»¯ liá»‡u trung bÃ¬nh, nhá». Äá»ƒ loáº¡i bá» cÃ¡c tá»« hiáº¿m ta sáº½ dÃ¹ng Counter Ä‘á»ƒ Ä‘áº¿m sá»‘ láº§n xuáº¥t hiá»‡n cá»§a cÃ¡c tá»« vÃ  tÃ¬m ra cÃ¡c tá»« Ã­t xuáº¥t hiá»‡n nháº¥t, sau Ä‘Ã³ loáº¡i bá» cÃ¡c tá»« nÃ y trong cÃ¢u nhÆ° Ä‘á»‘i vá»›i stop words.
      + EX:
          ```bash
          from collections import Counter
          # get word list
          word_list =list()
          for sentence in sentence_list:
            for word in sentence.split():
              word_list.append(word)
          word_counter = Counter(word_list)
          
          # for example, remove 10 rare words
          rare_word_list = set([word for (word, word_count) in word_counter.most_common()[:-10-1:-1]])
          print(rare_word_list)
          for i in range(len(sentence_list)):
            sentence_list[i] = " ".join([word for word in sentence_list[i].split() if word not in rare_word_list])
          ```
  - <b> Stemming & Lemmatization: </b>
    + Stemming lÃ  quÃ¡ trÃ¬nh biáº¿n Ä‘á»•i cÃ¡c tá»« vá» dáº¡ng gá»‘c cá»§a nÃ³ (vÃ­ dá»¥: connected, connection khi stemming thu Ä‘Æ°á»£c connect hay moved, move khi stemming thu Ä‘Æ°á»£c mov), lÆ°u Ã½ lÃ  stemming Ä‘Æ¡n giáº£n lÃ  loáº¡i bá» pháº§n cuá»‘i cá»§a tá»« tuy nhiÃªn dáº¡ng cá»§a tá»« thu Ä‘Æ°á»£c chÆ°a cháº¯c Ä‘Ã£ tá»“n táº¡i trong tiáº¿ng Anh (nhÆ° tá»« mov) (khÃ¡ tá»‡!!). ThÃ´ng thÆ°á»ng kÄ© thuáº­t nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chuáº©n hÃ³a bá»™ tá»« vá»±ng vÃ  ta cÅ©ng cáº§n cÃ¢n nháº¯c sá»­ dá»¥ng theo bÃ i toÃ¡n Ä‘áº·t ra. Trong thÆ° viá»‡n NLTK cÅ©ng cÃ³ há»— trá»£ thuáº­t toÃ¡n `Porter` Ä‘á»ƒ thá»±c hiá»‡n nhiá»‡m vá»¥ nÃ y.
    + EX:
        ```bash
        from nltk.stem.porter import PorterStemmer
        
        stemmer = PorterStemmer()
        stemmed_sentence_list = []
        for i in range(len(sentence_list)):
          stemmed_sentence_list.append(" ".join([stemmer.stem(word) for word in sentence_list[i].split()]))
        
        for i in range(100, 110):
          print(stemmed_sentence_list[i])
          print("-----")

        ```
        Khi quan sÃ¡t output, ta tháº¥y má»™t sá»‘ tá»« nhÆ° eruption -> erupt, astronomer -> astronom....
  
  - <b> Lemmatization </b>
      + Lemmatization vá» cÆ¡ báº£n lÃ  giá»‘ng vá»›i stemming khi nÃ³ loáº¡i bá» pháº§n Ä‘uÃ´i cá»§a tá»« Ä‘á»ƒ thu Ä‘Æ°á»£c gá»‘c tá»«, tuy nhiÃªn cÃ¡c gá»‘c tá»« á»Ÿ Ä‘Ã¢y Ä‘á»u thá»±c sá»± tá»‘n táº¡i chá»© khÃ´ng nhÆ° stemming (nhÆ° vÃ­ dá»¥ trÃªn thÃ¬ tá»« moved sau khi lemmatize sáº½ thu Ä‘Æ°á»£c move). Trong thÆ° viá»‡n NLTK sáº½ sá»­ dá»¥ng tá»« Ä‘iá»ƒn Wordnet Ä‘á»ƒ map theo cÃ¡c quy táº¯c (theo tÃ­nh cháº¥t cá»§a tá»«, tá»« lÃ  danh tá»«, Ä‘á»™ng tá»«, tráº¡ng tá»« hay tÃ­nh tá»«). Sá»­ dá»¥ng *part-of-speech tagging* `(nltk.pos_tag)` Ä‘á»ƒ thu Ä‘Æ°á»£c cÃ¡c tÃ­nh cháº¥t cá»§a tá»«.
      + EX:
          ```bash
          from nltk.stem import WordNetLemmatizer
          from nltk.corpus import wordnet
          
          wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV}
          lemmatizer = WordNetLemmatizer()
          lemmatized_sentence_list = []
          for i in range(len(sentence_list)):
            pos_tagged_sentence = nltk.pos_tag(sentence_list[i].split())
            lemmatized_sentence_list.append(" ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_sentence]))
          
          for i in range(100, 110):
            print(lemmatized_sentence_list[i])
            print("-----")

          ```
    - <b> Loáº¡i bá» cÃ¡c emoji: </b>
        + Äá»‘i vá»›i má»™t sá»‘ bÃ i toÃ¡n sá»­ dá»¥ng dá»¯ liá»‡u lÃ  cÃ¡c vÄƒn báº£n tá»« nhá»¯ng trang máº¡ng xÃ£ há»™i nhÆ° Twitter, Facebook,... thÃ¬ viá»‡c loáº¡i bá» cÃ¡c biá»ƒu tÆ°á»£ng cáº£m xÃºc - emoji lÃ  vÃ´ cÃ¹ng cáº§n thiáº¿t. Äá»ƒ loáº¡i bá» emoji ta sáº½ sá»­ dá»¥ng `regex, pattern` láº¥y tá»« repo nÃ y khÃ¡ Ä‘áº§y Ä‘á»§ vÃ  hoáº¡t Ä‘á»™ng tá»‘t.
        + EX:
            ```bash
            emoji_pattern = re.compile("["
                            u"\U0001F600-\U0001F64F"  # emoticons
                            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                            u"\U0001F680-\U0001F6FF"  # transport & map symbols
                            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                            u"\U00002500-\U00002BEF"  # chinese char
                            u"\U00002702-\U000027B0"
                            u"\U00002702-\U000027B0"
                            u"\U000024C2-\U0001F251"
                            u"\U0001f926-\U0001f937"
                            u"\U00010000-\U0010ffff"
                            u"\u2640-\u2642"
                            u"\u2600-\u2B55"
                            u"\u200d"
                            u"\u23cf"
                            u"\u23e9"
                            u"\u231a"
                            u"\ufe0f"  # dingbats
                            u"\u3030"
                            "]+", flags=re.UNICODE)
            emoji_example = "ğŸ˜… ğŸ‘  ğŸ˜† Without you by my side, ğŸ’“ ğŸ˜‰ I am not complete. You have given me the best of love, ğŸˆ and I want to be by your side forever. Thank you for giving my life that direction it needed. ğŸ’‹â€ Thank you for loving me unconditional. ğŸ’"
                  print(emoji_pattern.sub(r'', emoji_example))

            ```
    - <b>Loáº¡i bá» URL </b>
        + Khi crawl dá»¯ liá»‡u tá»« nhiá»u nguá»“n thÃ¬ khÃ´ng thá»ƒ trÃ¡nh khá»i viá»‡c cÃ¡c dá»¯ liá»‡u nÃ y sáº½ dÃ­nh cÃ¡c Ä‘Æ°á»ng dáº«n URL khÃ¡c nhau. Sá»­ dá»¥ng regex Ä‘á»ƒ loáº¡i bá» cÃ¡c URL.
        + EX:
            ```bash
            url_example = "You can read more about AI at https://viblo.asia/"
            url_pattern = re.compile(r'http\S+')
            print(url_pattern.sub(r'', url_example))

            ```

